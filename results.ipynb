{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using short QA pairs, we can achieve surprising results on the Cornell Dataset.\n",
    "\n",
    "## Import train/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data...\n"
     ]
    }
   ],
   "source": [
    "from process_cornell import ENCODING\n",
    "from process_cornell import TRAIN_PATH\n",
    "from process_cornell import VAL_PATH\n",
    "from utils import load_data\n",
    "\n",
    "\n",
    "print 'Preparing Data...'\n",
    "train = load_data(ENCODING, TRAIN_PATH)\n",
    "val = load_data(ENCODING, VAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from vocab import Vocab\n",
    "\n",
    "\n",
    "max_vocab_size = 20000\n",
    "\n",
    "tokens = []\n",
    "for q,a in train:\n",
    "    tokens.extend(chain(q, a))\n",
    "    \n",
    "counts = Counter(tokens)\n",
    "most_common = [token for token, count in counts.most_common(max_vocab_size)]\n",
    "vocab = Vocab()\n",
    "for token in most_common:\n",
    "    vocab.add_token(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to label encoded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(train)):\n",
    "    q, a = train[i]\n",
    "    q.append(Vocab.EOS_TOKEN)\n",
    "    a.append(Vocab.EOS_TOKEN)\n",
    "    a.insert(0, Vocab.SOS_TOKEN)\n",
    "    train[i] = (vocab.label_encode(q), vocab.label_encode(a))\n",
    "\n",
    "for i in xrange(len(val)):\n",
    "    q, a = val[i]\n",
    "    q.append(Vocab.EOS_TOKEN)\n",
    "    a.append(Vocab.EOS_TOKEN)\n",
    "    a.insert(0, Vocab.SOS_TOKEN)\n",
    "    val[i] = (vocab.label_encode(q), vocab.label_encode(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iters: 100, train loss: 4.81, val loss: 4.81, time: 4.87 s\n",
      "epoch: 1, iters: 200, train loss: 3.97, val loss: 4.11, time: 4.77 s\n",
      "epoch: 1, iters: 300, train loss: 3.78, val loss: 3.93, time: 4.73 s\n",
      "epoch: 2, iters: 400, train loss: 3.67, val loss: 3.87, time: 4.82 s\n",
      "epoch: 2, iters: 500, train loss: 3.54, val loss: 3.81, time: 4.73 s\n",
      "epoch: 3, iters: 600, train loss: 3.49, val loss: 3.78, time: 4.68 s\n",
      "epoch: 4, iters: 700, train loss: 3.40, val loss: 3.74, time: 4.76 s\n",
      "epoch: 4, iters: 800, train loss: 3.34, val loss: 3.66, time: 4.74 s\n",
      "epoch: 5, iters: 900, train loss: 3.27, val loss: 3.68, time: 4.77 s\n",
      "epoch: 5, iters: 1000, train loss: 3.22, val loss: 3.70, time: 4.77 s\n",
      "epoch: 6, iters: 1100, train loss: 3.17, val loss: 3.70, time: 5.11 s\n",
      "epoch: 7, iters: 1200, train loss: 3.12, val loss: 3.72, time: 4.76 s\n",
      "epoch: 7, iters: 1300, train loss: 3.06, val loss: 3.65, time: 4.76 s\n",
      "epoch: 8, iters: 1400, train loss: 3.03, val loss: 3.63, time: 4.77 s\n",
      "epoch: 8, iters: 1500, train loss: 2.96, val loss: 3.66, time: 4.79 s\n",
      "epoch: 9, iters: 1600, train loss: 2.94, val loss: 3.64, time: 4.73 s\n",
      "epoch: 9, iters: 1700, train loss: 2.87, val loss: 3.66, time: 4.72 s\n",
      "epoch: 10, iters: 1800, train loss: 2.82, val loss: 3.66, time: 4.73 s\n",
      "epoch: 11, iters: 1900, train loss: 2.78, val loss: 3.71, time: 4.70 s\n",
      "epoch: 11, iters: 2000, train loss: 2.74, val loss: 3.64, time: 4.73 s\n",
      "epoch: 12, iters: 2100, train loss: 2.71, val loss: 3.66, time: 4.78 s\n",
      "epoch: 12, iters: 2200, train loss: 2.66, val loss: 3.69, time: 4.81 s\n",
      "epoch: 13, iters: 2300, train loss: 2.60, val loss: 3.71, time: 4.86 s\n",
      "epoch: 14, iters: 2400, train loss: 2.56, val loss: 3.70, time: 4.63 s\n",
      "epoch: 14, iters: 2500, train loss: 2.49, val loss: 3.73, time: 4.73 s\n",
      "epoch: 15, iters: 2600, train loss: 2.47, val loss: 3.74, time: 4.56 s\n",
      "epoch: 15, iters: 2700, train loss: 2.41, val loss: 3.76, time: 4.52 s\n",
      "epoch: 16, iters: 2800, train loss: 2.33, val loss: 3.82, time: 4.54 s\n",
      "epoch: 16, iters: 2900, train loss: 2.29, val loss: 3.84, time: 4.51 s\n",
      "epoch: 17, iters: 3000, train loss: 2.23, val loss: 3.81, time: 4.48 s\n",
      "epoch: 18, iters: 3100, train loss: 2.17, val loss: 3.82, time: 4.53 s\n",
      "epoch: 18, iters: 3200, train loss: 2.10, val loss: 3.86, time: 4.48 s\n",
      "epoch: 19, iters: 3300, train loss: 2.03, val loss: 3.92, time: 4.55 s\n",
      "epoch: 19, iters: 3400, train loss: 1.98, val loss: 3.95, time: 4.49 s\n",
      "epoch: 20, iters: 3500, train loss: 1.91, val loss: 3.96, time: 4.48 s\n",
      "epoch: 21, iters: 3600, train loss: 1.88, val loss: 3.93, time: 4.55 s\n",
      "epoch: 21, iters: 3700, train loss: 1.82, val loss: 4.02, time: 4.53 s\n",
      "epoch: 22, iters: 3800, train loss: 1.75, val loss: 4.05, time: 4.42 s\n",
      "epoch: 22, iters: 3900, train loss: 1.69, val loss: 4.07, time: 4.46 s\n",
      "epoch: 23, iters: 4000, train loss: 1.62, val loss: 4.15, time: 4.54 s\n",
      "epoch: 23, iters: 4100, train loss: 1.57, val loss: 4.21, time: 4.72 s\n",
      "epoch: 24, iters: 4200, train loss: 1.53, val loss: 4.18, time: 4.41 s\n",
      "epoch: 25, iters: 4300, train loss: 1.46, val loss: 4.23, time: 4.38 s\n",
      "epoch: 25, iters: 4400, train loss: 1.39, val loss: 4.32, time: 4.49 s\n",
      "epoch: 26, iters: 4500, train loss: 1.33, val loss: 4.37, time: 4.37 s\n",
      "epoch: 26, iters: 4600, train loss: 1.29, val loss: 4.35, time: 4.40 s\n",
      "epoch: 27, iters: 4700, train loss: 1.24, val loss: 4.41, time: 4.41 s\n",
      "epoch: 28, iters: 4800, train loss: 1.19, val loss: 4.37, time: 4.36 s\n",
      "epoch: 28, iters: 4900, train loss: 1.14, val loss: 4.43, time: 4.41 s\n",
      "epoch: 29, iters: 5000, train loss: 1.09, val loss: 4.47, time: 4.41 s\n",
      "epoch: 29, iters: 5100, train loss: 1.04, val loss: 4.55, time: 4.52 s\n",
      "epoch: 30, iters: 5200, train loss: 1.01, val loss: 4.66, time: 4.43 s\n",
      "epoch: 31, iters: 5300, train loss: 0.96, val loss: 4.67, time: 4.51 s\n",
      "epoch: 31, iters: 5400, train loss: 0.92, val loss: 4.74, time: 4.38 s\n",
      "epoch: 32, iters: 5500, train loss: 0.87, val loss: 4.76, time: 4.49 s\n",
      "epoch: 32, iters: 5600, train loss: 0.87, val loss: 4.75, time: 4.38 s\n",
      "epoch: 33, iters: 5700, train loss: 0.81, val loss: 4.84, time: 4.42 s\n",
      "epoch: 33, iters: 5800, train loss: 0.79, val loss: 4.90, time: 4.39 s\n",
      "epoch: 34, iters: 5900, train loss: 0.74, val loss: 4.84, time: 4.39 s\n",
      "epoch: 35, iters: 6000, train loss: 0.71, val loss: 4.92, time: 4.40 s\n",
      "epoch: 35, iters: 6100, train loss: 0.68, val loss: 4.89, time: 4.38 s\n",
      "epoch: 36, iters: 6200, train loss: 0.66, val loss: 4.98, time: 4.43 s\n",
      "epoch: 36, iters: 6300, train loss: 0.61, val loss: 4.97, time: 4.38 s\n",
      "epoch: 37, iters: 6400, train loss: 0.61, val loss: 5.07, time: 4.38 s\n",
      "epoch: 38, iters: 6500, train loss: 0.58, val loss: 5.14, time: 4.43 s\n",
      "epoch: 38, iters: 6600, train loss: 0.55, val loss: 5.12, time: 4.38 s\n",
      "epoch: 39, iters: 6700, train loss: 0.52, val loss: 5.12, time: 4.40 s\n",
      "epoch: 39, iters: 6800, train loss: 0.51, val loss: 5.22, time: 4.43 s\n",
      "epoch: 40, iters: 6900, train loss: 0.49, val loss: 5.22, time: 4.39 s\n",
      "epoch: 40, iters: 7000, train loss: 0.48, val loss: 5.31, time: 4.43 s\n",
      "epoch: 41, iters: 7100, train loss: 0.45, val loss: 5.28, time: 4.36 s\n",
      "epoch: 42, iters: 7200, train loss: 0.45, val loss: 5.35, time: 4.43 s\n",
      "epoch: 42, iters: 7300, train loss: 0.44, val loss: 5.41, time: 4.50 s\n",
      "epoch: 43, iters: 7400, train loss: 0.41, val loss: 5.41, time: 4.39 s\n",
      "epoch: 43, iters: 7500, train loss: 0.40, val loss: 5.38, time: 4.39 s\n",
      "epoch: 44, iters: 7600, train loss: 0.38, val loss: 5.48, time: 4.46 s\n",
      "epoch: 45, iters: 7700, train loss: 0.39, val loss: 5.51, time: 4.41 s\n",
      "epoch: 45, iters: 7800, train loss: 0.37, val loss: 5.52, time: 4.46 s\n",
      "epoch: 46, iters: 7900, train loss: 0.36, val loss: 5.55, time: 4.40 s\n",
      "epoch: 46, iters: 8000, train loss: 0.35, val loss: 5.55, time: 4.46 s\n",
      "epoch: 47, iters: 8100, train loss: 0.34, val loss: 5.58, time: 4.45 s\n",
      "epoch: 47, iters: 8200, train loss: 0.33, val loss: 5.64, time: 4.38 s\n",
      "epoch: 48, iters: 8300, train loss: 0.32, val loss: 5.77, time: 4.47 s\n",
      "epoch: 49, iters: 8400, train loss: 0.32, val loss: 5.66, time: 4.43 s\n",
      "epoch: 49, iters: 8500, train loss: 0.31, val loss: 5.72, time: 4.38 s\n",
      "epoch: 50, iters: 8600, train loss: 0.32, val loss: 5.76, time: 4.44 s\n",
      "epoch: 50, iters: 8700, train loss: 0.29, val loss: 5.74, time: 4.40 s\n",
      "epoch: 51, iters: 8800, train loss: 0.29, val loss: 5.82, time: 4.37 s\n",
      "epoch: 52, iters: 8900, train loss: 0.30, val loss: 5.81, time: 4.43 s\n",
      "epoch: 52, iters: 9000, train loss: 0.29, val loss: 5.84, time: 4.38 s\n",
      "epoch: 53, iters: 9100, train loss: 0.29, val loss: 5.79, time: 4.38 s\n",
      "epoch: 53, iters: 9200, train loss: 0.27, val loss: 5.79, time: 4.53 s\n",
      "epoch: 54, iters: 9300, train loss: 0.29, val loss: 5.85, time: 4.37 s\n",
      "epoch: 55, iters: 9400, train loss: 0.28, val loss: 5.95, time: 4.45 s\n",
      "epoch: 55, iters: 9500, train loss: 0.28, val loss: 5.91, time: 4.39 s\n",
      "epoch: 56, iters: 9600, train loss: 0.28, val loss: 5.98, time: 4.39 s\n",
      "epoch: 56, iters: 9700, train loss: 0.27, val loss: 5.98, time: 4.44 s\n",
      "epoch: 57, iters: 9800, train loss: 0.26, val loss: 5.98, time: 4.57 s\n",
      "epoch: 57, iters: 9900, train loss: 0.26, val loss: 6.04, time: 4.42 s\n",
      "epoch: 58, iters: 10000, train loss: 0.26, val loss: 5.99, time: 4.47 s\n",
      "epoch: 59, iters: 10100, train loss: 0.26, val loss: 5.97, time: 4.41 s\n",
      "epoch: 59, iters: 10200, train loss: 0.26, val loss: 6.02, time: 4.46 s\n",
      "epoch: 60, iters: 10300, train loss: 0.26, val loss: 6.01, time: 4.40 s\n",
      "epoch: 60, iters: 10400, train loss: 0.25, val loss: 6.02, time: 4.39 s\n",
      "epoch: 61, iters: 10500, train loss: 0.26, val loss: 6.01, time: 4.41 s\n",
      "epoch: 62, iters: 10600, train loss: 0.26, val loss: 6.11, time: 4.38 s\n",
      "epoch: 62, iters: 10700, train loss: 0.25, val loss: 6.05, time: 4.38 s\n",
      "epoch: 63, iters: 10800, train loss: 0.25, val loss: 6.06, time: 4.44 s\n",
      "epoch: 63, iters: 10900, train loss: 0.24, val loss: 6.12, time: 4.44 s\n",
      "epoch: 64, iters: 11000, train loss: 0.24, val loss: 6.16, time: 4.44 s\n",
      "epoch: 64, iters: 11100, train loss: 0.23, val loss: 6.13, time: 4.38 s\n",
      "epoch: 65, iters: 11200, train loss: 0.24, val loss: 6.13, time: 4.40 s\n",
      "epoch: 66, iters: 11300, train loss: 0.24, val loss: 6.19, time: 4.42 s\n",
      "epoch: 66, iters: 11400, train loss: 0.24, val loss: 6.24, time: 4.37 s\n",
      "epoch: 67, iters: 11500, train loss: 0.24, val loss: 6.27, time: 4.46 s\n",
      "epoch: 67, iters: 11600, train loss: 0.24, val loss: 6.24, time: 4.40 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 68, iters: 11700, train loss: 0.24, val loss: 6.23, time: 4.56 s\n",
      "epoch: 69, iters: 11800, train loss: 0.23, val loss: 6.20, time: 4.49 s\n",
      "epoch: 69, iters: 11900, train loss: 0.23, val loss: 6.25, time: 4.41 s\n",
      "epoch: 70, iters: 12000, train loss: 0.23, val loss: 6.26, time: 4.48 s\n",
      "epoch: 70, iters: 12100, train loss: 0.23, val loss: 6.34, time: 4.54 s\n",
      "epoch: 71, iters: 12200, train loss: 0.24, val loss: 6.38, time: 4.41 s\n",
      "epoch: 71, iters: 12300, train loss: 0.23, val loss: 6.38, time: 4.45 s\n",
      "epoch: 72, iters: 12400, train loss: 0.23, val loss: 6.31, time: 4.40 s\n",
      "epoch: 73, iters: 12500, train loss: 0.24, val loss: 6.42, time: 4.40 s\n",
      "epoch: 73, iters: 12600, train loss: 0.28, val loss: 6.28, time: 4.44 s\n",
      "epoch: 74, iters: 12700, train loss: 0.28, val loss: 6.38, time: 4.52 s\n",
      "epoch: 74, iters: 12800, train loss: 0.26, val loss: 6.31, time: 4.41 s\n",
      "epoch: 75, iters: 12900, train loss: 0.24, val loss: 6.48, time: 4.45 s\n",
      "epoch: 76, iters: 13000, train loss: 0.23, val loss: 6.42, time: 4.40 s\n",
      "epoch: 76, iters: 13100, train loss: 0.23, val loss: 6.43, time: 4.45 s\n",
      "epoch: 77, iters: 13200, train loss: 0.23, val loss: 6.54, time: 4.48 s\n",
      "epoch: 77, iters: 13300, train loss: 0.22, val loss: 6.37, time: 4.42 s\n",
      "epoch: 78, iters: 13400, train loss: 0.23, val loss: 6.51, time: 4.47 s\n",
      "epoch: 78, iters: 13500, train loss: 0.23, val loss: 6.51, time: 4.41 s\n",
      "epoch: 79, iters: 13600, train loss: 0.21, val loss: 6.47, time: 4.41 s\n",
      "epoch: 80, iters: 13700, train loss: 0.22, val loss: 6.48, time: 4.45 s\n",
      "epoch: 80, iters: 13800, train loss: 0.22, val loss: 6.53, time: 4.42 s\n",
      "epoch: 81, iters: 13900, train loss: 0.22, val loss: 6.50, time: 4.47 s\n",
      "epoch: 81, iters: 14000, train loss: 0.22, val loss: 6.50, time: 4.41 s\n",
      "epoch: 82, iters: 14100, train loss: 0.22, val loss: 6.45, time: 4.39 s\n",
      "epoch: 83, iters: 14200, train loss: 0.22, val loss: 6.58, time: 4.46 s\n",
      "epoch: 83, iters: 14300, train loss: 0.21, val loss: 6.56, time: 4.42 s\n",
      "epoch: 84, iters: 14400, train loss: 0.22, val loss: 6.50, time: 4.42 s\n",
      "epoch: 84, iters: 14500, train loss: 0.23, val loss: 6.48, time: 4.45 s\n",
      "epoch: 85, iters: 14600, train loss: 0.21, val loss: 6.51, time: 4.42 s\n",
      "epoch: 86, iters: 14700, train loss: 0.22, val loss: 6.62, time: 4.45 s\n",
      "epoch: 86, iters: 14800, train loss: 0.22, val loss: 6.56, time: 4.44 s\n",
      "epoch: 87, iters: 14900, train loss: 0.21, val loss: 6.52, time: 4.41 s\n",
      "epoch: 87, iters: 15000, train loss: 0.22, val loss: 6.52, time: 4.44 s\n",
      "\n",
      "Total time: 0.19 hours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.cuda\n",
    "from torch.optim import Adam\n",
    "\n",
    "from models import NCM\n",
    "from vocab import Vocab\n",
    "from train import get_loss\n",
    "\n",
    "\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print_iters = 100\n",
    "iters = 15000\n",
    "\n",
    "batch_size = 64\n",
    "hidden_size = 256\n",
    "embedding_size = 32\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = NCM(len(vocab), embedding_size, hidden_size, num_layers).cuda()\n",
    "optimizer = Adam(model.parameters())\n",
    "torch.save(model.state_dict(), 'chat.init')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "iter_start_time = time.time()\n",
    "for i in xrange(1, iters + 1):\n",
    "    train_batch = [random.choice(train) for _ in xrange(batch_size)]\n",
    "    val_batch = [random.choice(val) for _ in xrange(batch_size)]\n",
    "\n",
    "    train_loss = get_loss(model, train_batch)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss = get_loss(model, val_batch, inference_only=True)\n",
    "\n",
    "    train_losses.append(train_loss.data[0])\n",
    "    val_losses.append(val_loss.data[0])\n",
    "\n",
    "    if i % print_iters == 0:\n",
    "        iter_end_time = time.time()\n",
    "\n",
    "        avg_train_loss = sum(train_losses[-print_iters:]) / print_iters\n",
    "        avg_val_loss = sum(val_losses[-print_iters:]) / print_iters\n",
    "\n",
    "        epoch = (batch_size * i) / len(train)\n",
    "\n",
    "        string = 'epoch: {}, iters: {}, train loss: {:.2f}, val loss: {:.2f}, time: {:.2f} s'\n",
    "        print string.format(epoch, i, avg_train_loss, avg_val_loss, iter_end_time - iter_start_time)\n",
    "\n",
    "        iter_start_time = time.time()\n",
    "        \n",
    "    if i == 2000:  # val minimum\n",
    "        torch.save(model.state_dict(), 'chat.min')\n",
    "\n",
    "torch.save(model.state_dict(), 'chat.final')\n",
    "\n",
    "end_time = time.time()\n",
    "seconds_per_hour = 60.**2\n",
    "print '\\nTotal time: {:.2f} hours\\n'.format((end_time - start_time) / seconds_per_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"2f85fb27-edc6-4d18-b870-ad327741b410\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"0feeeaae-795d-40ff-b483-5d5df8a6d534\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "6851d1d4-cf93-4ea9-861c-44e8533e581a"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "p = figure(y_axis_label='perplexity', x_axis_label='iters')\n",
    "p.line(range(iters), train_losses, legend='train')\n",
    "p.line(range(iters), val_losses, legend='val', color='orange')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the chat results at 3 different points in the training: before training, at the val loss minimum and at the end of training.\n",
    "\n",
    "**Note:** Each question has 3 answers, one from each stage of training. The first, second and third answers are from before training, at the val loss minimum and at the end of training, respectively.\n",
    "\n",
    "## Wait why do you care...\n",
    "\n",
    "Perplexity is a rather simple/poor measure for this sort of task; just because a given answer, word for word,| doesn't match an expected answer doesn't mean it isn't a good answer. \n",
    "\n",
    "That being said, maybe the model's responses, on the validation set, do become better despite an incrasing val loss; I wanted to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Can you say no?\n",
      "A: extra what- what- scars scars scars feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars feature\n",
      "A: no .\n",
      "A: what ?\n",
      "\n",
      "\n",
      "Q: What is your name?\n",
      "A: cecil sutphin dead assignment scars scars reputation feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars\n",
      "A: i do n't know .\n",
      "A: bedevere , my liege .\n",
      "\n",
      "\n",
      "Q: Where are we going?\n",
      "A: kansas kansas scars scars scars scars reputation feature reputation feature feature scars reputation feature feature reputation feature lady feature scars scars feature reputation\n",
      "A: i do n't know .\n",
      "A: nowhere .\n",
      "\n",
      "\n",
      "Q: What color is the sky?\n",
      "A: cecil sutphin assignment scars scars scars reputation feature reputation feature feature reputation feature lady feature scars scars feature reputation feature reputation feature feature\n",
      "A: i do n't know .\n",
      "A: nothing .\n",
      "\n",
      "\n",
      "Q: How tall are you?\n",
      "A: in scars scars scars feature reputation feature reputation feature feature lady scars feature scars dog bruno bruno dog bruno bruno dog bruno bruno\n",
      "A: yes .\n",
      "A: fine , fine .\n",
      "\n",
      "\n",
      "Q: 2 plus 2 is\n",
      "A: cecil sutphin dead assignment scars scars reputation feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars\n",
      "A: sure .\n",
      "A: 're house .\n",
      "\n",
      "\n",
      "Q: Will google hire me?\n",
      "A: cecil sutphin assignment scars scars scars reputation feature reputation feature feature reputation feature lady feature scars scars feature reputation feature reputation feature feature\n",
      "A: i do n't know .\n",
      "A: i told my .\n",
      "\n",
      "\n",
      "Q: Tell me the alphabet\n",
      "A: cecil bruno brazil brazil brazil brazil ralph ralph fighter in scars scars scars reputation feature feature reputation feature lady feature scars scars feature\n",
      "A: what ?\n",
      "A: i sure .\n",
      "\n",
      "\n",
      "Q: What is your gender?\n",
      "A: cecil sutphin assignment scars scars scars reputation feature reputation feature feature reputation feature lady feature scars scars feature reputation feature reputation feature feature\n",
      "A: i do n't know .\n",
      "A: a body .\n",
      "\n",
      "\n",
      "Q: What is love?\n",
      "A: cecil sutphin dead assignment scars scars reputation feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars\n",
      "A: the chest-cutter .\n",
      "A: a little .\n",
      "\n",
      "\n",
      "Q: Tell me a joke.\n",
      "A: in bruno bruno brazil brazil brazil brazil ralph ralph fighter in scars scars scars reputation feature feature reputation feature lady feature scars scars\n",
      "A: what ?\n",
      "A: tell me ?\n",
      "\n",
      "\n",
      "Q: What day is it?\n",
      "A: kansas scars scars scars feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars feature scars dog\n",
      "A: i do n't know .\n",
      "A: tuesday .\n",
      "\n",
      "\n",
      "Q: Who are you?\n",
      "A: in scars scars scars feature reputation feature reputation feature feature lady scars feature scars dog bruno bruno dog bruno bruno dog bruno bruno\n",
      "A: i 'm not .\n",
      "A: i 'm right .\n",
      "\n",
      "\n",
      "Q: How do you feel?\n",
      "A: extra what- what- cooked-off scars scars scars reputation feature reputation feature feature reputation feature lady feature scars scars feature reputation feature reputation feature\n",
      "A: no .\n",
      "A: ca n't complain .\n",
      "\n",
      "\n",
      "Q: Are you tired?\n",
      "A: classic kansas scars scars scars reputation feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars feature\n",
      "A: yes .\n",
      "A: yes .\n",
      "\n",
      "\n",
      "Q: Are you hungry?\n",
      "A: in scars scars scars feature reputation feature reputation feature feature lady scars feature scars dog bruno bruno dog bruno bruno dog bruno bruno\n",
      "A: yes .\n",
      "A: yes .\n",
      "\n",
      "\n",
      "Q: Are you strong?\n",
      "A: what- what- scars scars scars feature reputation feature reputation feature feature scars scars feature reputation feature reputation feature feature lady scars feature scars\n",
      "A: yes .\n",
      "A: what ?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from process_cornell import tokenize\n",
    "from utils import chat\n",
    "\n",
    "questions = ['Can you say no?', 'What is your name?', 'Where are we going?', 'What color is the sky?', \n",
    "            'How tall are you?', '2 plus 2 is', 'Will google hire me?', 'Tell me the alphabet', \n",
    "            'What is your gender?', 'What is love?', 'Tell me a joke.', 'What day is it?', 'Who are you?', \n",
    "            'How do you feel?', 'Are you tired?', 'Are you hungry?', 'Are you strong?']\n",
    "\n",
    "for q in questions:\n",
    "    print 'Q: {}'.format(q)\n",
    "    for path in mdl_paths:\n",
    "        model = NCM(len(vocab), embedding_size, hidden_size, num_layers).cuda()\n",
    "        model.load_state_dict(torch.load(path))\n",
    "\n",
    "        a = chat(q, tokenize, model, vocab).split()\n",
    "        a = ' '.join(a[1:len(a) - 1])  # rm start & stop token\n",
    "        print 'A: {}'.format(a.encode('utf-8'))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The results from the untrained model are clearly gibberish as expected.\n",
    "Looking at the other two checkpoints, the responses seem to be roughly equally sensible. However, the responses at the end of training tend to be more specific and interesting. On the other hand, responses from the model, at the min val loss, tend to be evasive and limited. \n",
    "\n",
    "\n",
    "# Where to go from here?\n",
    "\n",
    "The next step would be to move to the opensubtitles dataset. This has two benefits:\n",
    "  1. Larger/richer dataset\n",
    "  2. Used in the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just for fun, here is a particularly quirky model I came across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Can you say no?\n",
      "A: no .\n",
      "\n",
      "Q: What is your name?\n",
      "A: bedevere , sir .\n",
      "\n",
      "Q: Where are we going?\n",
      "A: to take a picture .\n",
      "\n",
      "Q: What color is the sky?\n",
      "A: i 'm not finished .\n",
      "\n",
      "Q: How tall are you?\n",
      "A: not good .\n",
      "\n",
      "Q: 2 plus 2 is\n",
      "A: ?\n",
      "\n",
      "Q: Will google hire me?\n",
      "A: for a second ?\n",
      "\n",
      "Q: Tell me the alphabet\n",
      "A: tell much .\n",
      "\n",
      "Q: What is your gender?\n",
      "A: a funeral .\n",
      "\n",
      "Q: What is love?\n",
      "A: for .\n",
      "\n",
      "Q: Tell me a joke.\n",
      "A: tell me .\n",
      "\n",
      "Q: What day is it?\n",
      "A: it .\n",
      "\n",
      "Q: Who are you?\n",
      "A: i 'm the creator .\n",
      "\n",
      "Q: How do you feel?\n",
      "A: i 'm fine .\n",
      "\n",
      "Q: Are you tired?\n",
      "A: what ?\n",
      "\n",
      "Q: Are you hungry?\n",
      "A: what ?\n",
      "\n",
      "Q: Are you strong?\n",
      "A: yes .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    path = 'chat_6174_32_256_2.mdl'\n",
    "    model = NCM(len(vocab), embedding_size, hidden_size, num_layers).cuda()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    a = chat(q, tokenize, model, vocab).split()\n",
    "    a = ' '.join(a[1:len(a) - 1])  # rm start & stop token\n",
    "    print 'Q: {}\\nA: {}\\n'.format(q, a.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
